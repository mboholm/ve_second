{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with static distributional vectors is the difficulty of distinguishing between different *word senses*. We will continue our exploration of word vectors by considering *trainable vectors* or *word embeddings* for Word Sense Disambiguation (WSD).\n",
    "\n",
    "The goal of word sense disambiguation is to train a model to find the sense of a word (homonyms of a word-form). For example, the word \"bank\" can mean \"sloping land\" or \"financial institution\". \n",
    "\n",
    "(a) \"I deposited my money in the **bank**\" (financial institution)\n",
    "\n",
    "(b) \"I swam from the river **bank**\" (sloping land)\n",
    "\n",
    "In case a) and b) we can determine that the meaning of \"bank\" based on the *context*. To utilize context in a semantic model we use *contextualized word representations*. Previously we worked with *static word representations*, i.e. the representation does not depend on the context. To illustrate we can consider sentences (a) and (b), the word **bank** would have the same static representation in both sentences, which means that it becomes difficult for us to predict its sense. What we want is to create representations that depend on the context, i.e. *contextualized embeddings*. \n",
    "\n",
    "We will create contextualized embeddings with Recurrent Neural Networks. You can read more about recurrent neural netoworks [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Your overall task in this lab is to create a neural network model that can disambiguate the word sense of 30 different words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: **MAX BOHOLM**\n",
    "\n",
    "*Second attempt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.7.0+cu101\n"
     ]
    }
   ],
   "source": [
    "# first we import some packages that we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# and define our device\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our hyperparameters for Part A (add more when/if you need them)\n",
    "a_batch_size = 16\n",
    "a_learning_rate = 0.001\n",
    "a_epochs = 8\n",
    "a_hidden = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with data\n",
    "\n",
    "A central part of any machine learning system is the data we're working with. In this section we will split the data (the dataset is located here: ``wsd-data/wsd_data.txt``) into a training set and a test set. We will also create a baseline to compare our model against. Finally, we will use TorchText to transform our data (raw text) into a convenient format that our neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contain different word sense for 30 different words. The data is organized as follows (values separated by tabs): \n",
    "- Column 1: word-sense\n",
    "- Column 2: word-form\n",
    "- Column 3: index of word\n",
    "- Column 4: white-space tokenized context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "Your first task is to seperate the data into a *training set* and a *test set*. The training set should contain 80% of the examples and the test set the remaining 20%. The examples for the test/training set should be selected **randomly**. Save each dataset into a .csv file for loading later. **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Looks good! **2 marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data_split(path_to_dataset, directory_for_output=\"wsd_data\", train_frac=0.8):\n",
    "\n",
    "    with open(path_to_dataset, mode=\"r\") as f:\n",
    "        data=[example for example in f.read().split(\"\\n\") if len(example.split(\"\\t\")) == 4]\n",
    "        \n",
    "    #data=data[:20000] #OBS!\n",
    "\n",
    "    random.shuffle(data)\n",
    "    n_train = int(len(data)*train_frac)\n",
    "    train=data[:n_train]\n",
    "    test=data[n_train:]\n",
    "    \n",
    "    with open(f\"{directory_for_output}/train.csv\", mode=\"w\") as f:\n",
    "        f.write(\"\\n\".join(train))\n",
    "    with open(f\"{directory_for_output}/test.csv\", mode=\"w\") as f:\n",
    "        f.write(\"\\n\".join(test))\n",
    "\n",
    "data_split(\"wsd_data/wsd_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a baseline\n",
    "\n",
    "Your second task is to create a *baseline* for the task. A baseline is a \"reality check\" for a model, given a very simple heuristic/algorithmic/model solution to the problem, can our neural network perform better than this?\n",
    "The baseline you are to create is the \"most common sense\" (MCS) baseline. For each word form, find the most commonly assigned sense to the word, and label a words with that sense. **[2 marks]**\n",
    "\n",
    "E.g. In a fictional dataset, \"bank\" have two senses, \"financial institution\" which occur 5 times and \"side of river\" 3 times. Thus, all 8 occurences of bank is labeled \"financial institution\" and this yields an MCS accuracy of 5/8 = 62.5%. If a model obtain a higher score than this, we can conclude that the model *at least* is better than selecting the most frequent word sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Looks good! **2 marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcs_baseline(path_to_data=\"wsd_data/wsd_data.txt\"): #baseline on the test data alone?\n",
    "    with open(path_to_data, mode=\"r\") as f:\n",
    "        data=[tuple(line.split(\"\\t\")[:2]) for line in f.read().split(\"\\n\") if line != \"\"]\n",
    "   \n",
    "    counts={lemma:{} for lemma in [lemma for sense, lemma in data]}\n",
    "    for sense, lemma in data:\n",
    "        if sense in counts[lemma]:\n",
    "            counts[lemma][sense]+=1\n",
    "        else:\n",
    "            counts[lemma][sense]=1  \n",
    "    \n",
    "    baseline={lemma:{} for lemma in counts.keys()}\n",
    "    for lemma in counts.keys():\n",
    "        my_top_sense = list(counts[lemma].keys())[0]\n",
    "        for sense in counts[lemma].keys():\n",
    "            if counts[lemma][sense] > counts[lemma][my_top_sense]:\n",
    "                my_top_sense = sense\n",
    "        total=sum(counts[lemma].values())\n",
    "        baseline[lemma][\"sense\"]=my_top_sense\n",
    "        baseline[lemma][\"accuracy\"]=counts[lemma][my_top_sense] / total\n",
    "        baseline[lemma][\"no_of_senses\"]=len(counts[lemma].keys())\n",
    "    \n",
    "    return baseline\n",
    "\n",
    "my_baseline = mcs_baseline()\n",
    "#print(my_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data iterators\n",
    "\n",
    "To train a neural network, we first need to prepare the data. This involves converting words (and labels) to a number, and organizing the data into batches. We also want the ability to shuffle the examples such that they appear in a random order.  \n",
    "\n",
    "To do all of this we will use the torchtext library (https://torchtext.readthedocs.io/en/latest/index.html). In addition to converting our data into numerical form and creating batches, it will generate a word and label vocabulary, and data iterators than can sort and shuffle the examples. \n",
    "\n",
    "Your task is to create a dataloader for the training and test set you created previously. So, how do we go about doing this?\n",
    "\n",
    "1) First we create a ``Field`` for each of our columns. A field is a function which tokenize the input, keep a dictionary of word-to-numbers, and fix paddings. So, we need four fields, one for the word-sense, one for the position, one for the lemma and one for the context. \n",
    "\n",
    "2) After we have our fields, we need to process the data. For this we use the ``TabularDataset`` class. We pass the name and path of the training and test files we created previously, then we assign which field to use in each column. The result is that each column will be processed by the field indicated. So, the context column will be tokenized and processed by the context field and so on. \n",
    "\n",
    "3) After we have processed the dataset we need to build the vocabulary, for this we call the function ``build_vocab()`` on the different ``Fields`` with the output from ``TabularDataset`` as input. This looks at our dataset and creates the necessary vocabularies (word-to-number mappings). \n",
    "\n",
    "4) Finally, the last step. In the last step we load the data objects given by the ``TabularDataset`` and pass it to the ``BucketIterator`` class. This class will organize our examples into batches and shuffle them around (such that for each epoch the model observe the examples in a different order). When we are done with this we can let our function return the data iterators and vocabularies, then we are ready to train and test our model!\n",
    "\n",
    "Implement the dataloader. [**2 marks**]\n",
    "\n",
    "*hint: for TabularDataset and BucketIterator use the class function splits()* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Looks good! **2 marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "\n",
    "def dataloader(directory=\"wsd_data\",\n",
    "               train_file=\"train.csv\",\n",
    "               test_file=\"test.csv\",\n",
    "               batch=a_batch_size):\n",
    "    \n",
    "    whitespacer = lambda x: x.split(' ') #from: https://canvas.gu.se/files/4597768/download?download_frd=1\n",
    "    to_int      = lambda x: [int(x[0])]\n",
    "    \n",
    "    SENSE = Field(batch_first = True)\n",
    "\n",
    "    LEMMA = Field(batch_first = True) \n",
    "    \n",
    "    INDEX = Field(batch_first   = True,\n",
    "                  use_vocab     = False,\n",
    "                  preprocessing = to_int\n",
    "                 ) \n",
    "    \n",
    "    CONTEXT = Field(tokenize    = whitespacer,\n",
    "                    lower       = True,\n",
    "                    batch_first = True,\n",
    "                    init_token  = \"<start>\", \n",
    "                    eos_token   = \"<end>\"\n",
    "                   ) \n",
    "    \n",
    "    my_fields = [(\"sense\", SENSE),\n",
    "                 (\"lemma\", LEMMA),\n",
    "                 (\"index\", INDEX),\n",
    "                 (\"context\", CONTEXT)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = 'train.csv',\n",
    "                                        test   = 'test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'¤'}) \n",
    "                                        #\"¤\" not in data\n",
    "    SENSE.build_vocab(train) #labels\n",
    "    LEMMA.build_vocab(train) #lemmas \n",
    "    CONTEXT.build_vocab(train) #Vocabulary\n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.context),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, CONTEXT.vocab, SENSE.vocab, LEMMA.vocab  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Creating and running a Neural Network for WSD\n",
    "\n",
    "In this section we will create and run a neural network to predict word senses based on *contextualized representations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will use a bidirectional Long-Short-Term Memory (LSTM) network to create a representation for the sentences and a Linear classifier to predict the sense of each word.\n",
    "\n",
    "When we initialize the model, we need a few things:\n",
    "\n",
    "    1) An embedding layer: a dictionary from which we can obtain word embeddings\n",
    "    2) A LSTM-module to obtain contextual representations\n",
    "    3) A classifier that compute scores for each word-sense given *some* input\n",
    "\n",
    "\n",
    "The general procedure is the following:\n",
    "\n",
    "    1) For each word in the sentence, obtain word embeddings\n",
    "    2) Run the embedded sentences through the RNN\n",
    "    3) Select the appropriate hidden state\n",
    "    4) Predict the word-sense \n",
    "\n",
    "**Suggestion for efficiency:**  *Use a low dimensionality (32) for word embeddings and the LSTM when developing and testing the code, then scale up when running the full training/tests*\n",
    "    \n",
    "Your tasks will be to create two different models (both follow the two outlines described above), described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL 1: Ambigious Word Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first approach to WSD, you are to select the index of our target word (column 3 in the dataset) and predict the word sense. **[5 marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: This needs some work, a problem I see here is that you predict the sense for every word, then select the prediction for the ambiguous word. This is problematic because when backpropagating, the classification will update it's prediction on ALL words in the sentence (such that it becomes better for all words) not only the ambiguous word which is what we want. \n",
    "\n",
    "So, you should change `classifications = self.classifier(contextualized_embedding)` to something like `classifications = self.classifier(contextualized_embedding[SELECT_AMBIGUOUS_WORD])`. You sorta have the key to this already when you're selecting the *predictions* of the ambiguous word.\n",
    "\n",
    "**2 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Old code for Model1*\n",
    "\n",
    "```\n",
    "class WSDModel_approach1(nn.Module):\n",
    "    def __init__(self, voc_size, hidden, n_labels):  \n",
    "        super(WSDModel_approach1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(voc_size, hidden)\n",
    "        self.rnn = nn.LSTM(hidden, hidden, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden*2, n_labels) \n",
    "  \n",
    "    def forward(self, batch, index):\n",
    "        embeddings = self.embeddings(batch)\n",
    "        contextualized_embedding, *_ = self.rnn(embeddings)\n",
    "        classifications = self.classifier(contextualized_embedding)\n",
    "        \n",
    "        ### NOTES ON TENSOR TRANSFORMATIONS ###\n",
    "        # 1. Add 1 to the index input since we have a start-token of he sequence\n",
    "        # 2. We make a vector (1D tensor) of the batch * index tensor\n",
    "        # 3. We take the first batch example to \"build upon\" `predictions = classifications[0, index_mod[0], :].unsqueeze(0)`\n",
    "        # 4. We iterate over the remaining batch examples to build the full output\n",
    "        #######################################\n",
    "        \n",
    "        index_mod = torch.add(index.squeeze(), 1)\n",
    "        predictions = classifications[0, index_mod[0], :].unsqueeze(0)\n",
    "        print(predictions.size())\n",
    "        for counter, index_at_count in enumerate(index_mod[1:], start=1):\n",
    "            to_add = classifications[counter, index_at_count, :].unsqueeze(0)\n",
    "            print(to_add.size())\n",
    "            predictions = torch.cat((predictions, to_add)) #dim=0 by default\n",
    "            \n",
    "        print(predictions.size())\n",
    "       \n",
    "        return predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-WORK SUMMER 2021\n",
    "\n",
    "class WSDModel_approach1(nn.Module):\n",
    "    def __init__(self, voc_size, hidden, n_labels):  \n",
    "        super(WSDModel_approach1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(voc_size, hidden)\n",
    "        self.rnn = nn.LSTM(hidden, hidden, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden*2, n_labels) \n",
    "  \n",
    "    def forward(self, batch, index):\n",
    "        embeddings = self.embeddings(batch)\n",
    "        contextualized_embedding, *_ = self.rnn(embeddings)\n",
    "        \n",
    "        #      VVVVV HERE IS MY NEW IDEA VVVV\n",
    "        # First, we rebuild a batch of vectors representing the ambigious word\n",
    "        # --- Add 1 to the index input since we have a start-token of the sequence\n",
    "        index_mod = torch.add(index.squeeze(), 1)\n",
    "        # --- We take the first batch example to \"build upon\"\n",
    "        projection = contextualized_embedding[0, index_mod[0], :].unsqueeze(0)\n",
    "        # --- We iterate over the remaining batch examples to build the full output\n",
    "        for counter, index_at_count in enumerate(index_mod[1:], start=1):\n",
    "            to_add = contextualized_embedding[counter, index_at_count, :].unsqueeze(0)\n",
    "            #print(to_add.size())\n",
    "            projection = torch.cat((projection, to_add)) #dim=0 by default\n",
    "\n",
    "        # Second, we predict labels \n",
    "        #print(projection.size())\n",
    "        predictions = self.classifier(projection)\n",
    "        #print(predictions.size())\n",
    "       \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL 2: Sentence Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second approach to WSD, you are to predict the word sense based on the final hidden state given by the RNN. **[5 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Same comment as for the previous approach, also note that the LSTM gives you three outputs, `TOKEN_REPRESENTATIONS, (final_hidden, final_cell)`. You can use the `final_hidden` here instead of the `<end>` token. **2 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Old code for Model2*\n",
    "```\n",
    "class WSDModel_approach2(nn.Module):\n",
    "    def __init__(self, voc_size, hidden, n_labels):  \n",
    "        super(WSDModel_approach2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(voc_size, hidden)\n",
    "        self.rnn = nn.LSTM(hidden, hidden, bidirectional=True, batch_first=True) #bidirectional?\n",
    "        self.classifier = nn.Linear(hidden*2, n_labels) \n",
    "  \n",
    "    def forward(self, batch, index): #index is dummy in model2 \n",
    "   \n",
    "        embeddings = self.embeddings(batch)\n",
    "      \n",
    "        contextualized_embedding, *_ = self.rnn(embeddings)\n",
    "     \n",
    "        classifications = self.classifier(contextualized_embedding)\n",
    "        \n",
    "        ### NOTES ON TENSOR TRANSFORMATIONS ###\n",
    "        # 1. Identify index of <end> i.e. key 3\n",
    "        # 2. Use this list of indecies in the same way as \"index\" of the dataset (above)\n",
    "        #######################################\n",
    "        \n",
    "        end=torch.tensor(3, device=device)\n",
    "        end_index=(end == batch).nonzero(as_tuple=True)[1]\n",
    "        \n",
    "        predictions = classifications[0, end_index[0], :].unsqueeze(0)\n",
    "        \n",
    "        for counter, index_at_count in enumerate(end_index[1:], start=1):\n",
    "            to_add = classifications[counter, index_at_count, :].unsqueeze(0)\n",
    "            predictions = torch.cat((predictions, to_add)) #dim=0 by default\n",
    "       \n",
    "        return predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-WORK SUMMER 2021\n",
    "\n",
    "class WSDModel_approach2(nn.Module):\n",
    "    def __init__(self, voc_size, hidden, n_labels):  \n",
    "        super(WSDModel_approach2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(voc_size, hidden)\n",
    "        self.rnn = nn.LSTM(hidden, hidden, bidirectional=True, batch_first=True) #bidirectional?\n",
    "        self.classifier = nn.Linear(hidden*2, n_labels) \n",
    "  \n",
    "    def forward(self, batch, index): #index is dummy in model2 \n",
    "   \n",
    "        embeddings = self.embeddings(batch)\n",
    "    \n",
    "        #   VVVV HERE IS THE NEW IDEA VVVV\n",
    "      \n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(embeddings)\n",
    "        \n",
    "        # Structure of hidden_final: (D∗num_layers, batch_size, dimension_hidden), where D=2, if bidirectional=True, as it is in our case (see https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "        # We want to combine hidden_final[0] with hidden_final[1], resuling in a tensor shaped [batch_size, dimension_hidden*2]\n",
    "        \n",
    "        #print(hidden_final.shape)\n",
    "        projection = torch.cat((hidden_final[0], hidden_final[1]), 1)        \n",
    "        #print(projection.shape)\n",
    "        \n",
    "        predictions = self.classifier(projection)\n",
    "        \n",
    "        return predictions       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the model\n",
    "\n",
    "Now we are ready to train and test our model. What we need now is a loss function, an optimizer, and our data. \n",
    "\n",
    "- First, create the loss function and the optimizer.\n",
    "- Next, we iterate over the number of epochs (i.e. how many times we let the model see our data). \n",
    "- For each epoch, iterate over the dataset (``train_iter``) to obtain batches. Use the batch as input to the model, and let the model output scores for the different word senses.\n",
    "- For each model output, calculate the loss (and print the loss) on the output and update the model parameters.\n",
    "- Reset the gradients and repeat.\n",
    "- After all epochs are done, test your trained model on the test set (``test_iter``) and calculate the total and per-word-form accuracy of your model.\n",
    "\n",
    "Implement the training and testing of the model **[4 marks]**\n",
    "\n",
    "**Suggestion for efficiency:** *when developing your model, try training and testing the model on one or two batches (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Looks good! **4 marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.426624774932861\n",
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.427377223968506\n",
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.432816505432129\n",
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.369637489318848\n",
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.336387634277344\n",
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.331856727600098\n",
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.5099921226501465\n",
      "torch.Size([2, 16, 256])\n",
      "torch.Size([16, 512])\n",
      "0 5.370046615600586\n"
     ]
    }
   ],
   "source": [
    "#Note: I have splitted training and testing into separate cells\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "train_iter, test_iter, vocab, labels, lemmas = dataloader()\n",
    "\n",
    "model = WSDModel_approach1(voc_size = len(vocab),\n",
    "                           hidden   = a_hidden, \n",
    "                           n_labels = len(labels))\n",
    "\n",
    "model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=a_learning_rate)\n",
    "\n",
    "\n",
    "for e in range(a_epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        \n",
    "        sentence = batch.context\n",
    "        index = batch.index\n",
    "        label = batch.sense\n",
    "      \n",
    "        output_from_model = model(sentence, index)\n",
    "        #assert False\n",
    "        \n",
    "        loss = loss_function(output_from_model, label.squeeze()) # \"output\" from model is \"input\" to CEL\n",
    "        \n",
    "        #Note: code below adopted from previous assignment\n",
    "        total_loss += loss.item()\n",
    "        print(i, total_loss/(i+1), end='\\r') \n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # update parameters\n",
    "        optimizer.zero_grad # reset gradients\n",
    "        \n",
    "        #break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "EVALUATION\n",
      "Overall accuracy: 0.013.\n",
      "Lemma         \tAcc.\tBaseL.\tGood?\tNo. senses\n",
      "see.v         \t0.04\t0.63\tNo\t11\n",
      "line.n        \t0.04\t0.85\tNo\t11\n",
      "keep.v        \t0.0\t0.39\tNo\t11\n",
      "follow.v      \t0.01\t0.15\tNo\t11\n",
      "hold.v        \t0.0\t0.15\tNo\t11\n",
      "serve.v       \t0.01\t0.16\tNo\t9\n",
      "force.n       \t0.0\t0.16\tNo\t8\n",
      "lead.v        \t0.0\t0.18\tNo\t8\n",
      "build.v       \t0.01\t0.21\tNo\t10\n",
      "bring.v       \t0.0\t0.21\tNo\t8\n",
      "extend.v      \t0.0\t0.18\tNo\t7\n",
      "find.v        \t0.0\t0.23\tNo\t10\n",
      "case.n        \t0.0\t0.2\tNo\t8\n",
      "position.n    \t0.01\t0.2\tNo\t6\n",
      "security.n    \t0.0\t0.2\tNo\t7\n",
      "life.n        \t0.0\t0.22\tNo\t9\n",
      "national.a    \t0.0\t0.2\tNo\t6\n",
      "time.n        \t0.0\t0.28\tNo\t5\n",
      "professional.a\t0.0\t0.22\tNo\t5\n",
      "order.n       \t0.02\t0.22\tNo\t5\n",
      "regular.a     \t0.0\t0.22\tNo\t8\n",
      "place.n       \t0.09\t0.24\tNo\t7\n",
      "physical.a    \t0.03\t0.24\tNo\t6\n",
      "point.n       \t0.0\t0.36\tNo\t8\n",
      "bad.a         \t0.01\t0.61\tNo\t4\n",
      "common.a      \t0.0\t0.25\tNo\t4\n",
      "critical.a    \t0.02\t0.27\tNo\t5\n",
      "major.a       \t0.02\t0.3\tNo\t4\n",
      "active.a      \t0.0\t0.32\tNo\t5\n",
      "positive.a    \t0.0\t0.35\tNo\t5\n",
      "========================================\n",
      "\n",
      "Correlation of Accuracy and Baseline: 0.345\n",
      "Correlation of Accuracy and No. of senses: 0.058\n"
     ]
    }
   ],
   "source": [
    "# evaluate model after all epochs are completed\n",
    "import numpy as np\n",
    "\n",
    "def select(vector):\n",
    "    \"\"\"Selects the index of the top value in a vector.\"\"\"\n",
    "    top_value=0\n",
    "    no_one=0 #index of top value\n",
    "    for index, value in enumerate(vector):\n",
    "        if value > top_value:\n",
    "            top_value=value\n",
    "            no_one=index\n",
    "    return no_one\n",
    "\n",
    "correct_set = []\n",
    "correct_per_word = {lemma:[] for lemma in [lemmas.itos[x] for x in range(len(lemmas))]}\n",
    "model.eval() #evaluation mode\n",
    "\n",
    "for i, batch in enumerate(test_iter):\n",
    "    print(f\"{round((i/len(test_iter))*100, 3)} %\", end=\"\\r\")\n",
    "    sentence = batch.context\n",
    "    index = batch.index\n",
    "    label = batch.sense\n",
    "    lemma = batch.lemma\n",
    "    \n",
    "    output = model(sentence, index)\n",
    "    \n",
    "    my_probs = F.softmax(output, dim=1)\n",
    "    index_of_top_prob = [select(x) for x in my_probs]\n",
    "    predicted_label = [labels.itos[x] for x in index_of_top_prob]\n",
    "\n",
    "    for i in range(label.shape[0]):\n",
    "        true_label = labels.itos[label[i][0]]\n",
    "        this_lemma = lemmas.itos[lemma[i][0]]\n",
    "        if true_label == predicted_label[i]:\n",
    "            correct_set.append(1)\n",
    "            correct_per_word[this_lemma].append(1)\n",
    "        else:\n",
    "            correct_set.append(0)\n",
    "            correct_per_word[this_lemma].append(0)\n",
    "\n",
    "accuracy = sum(correct_set) / len(correct_set)\n",
    "\n",
    "accuracy_per_word = {lemma:0 for lemma in correct_per_word.keys()}\n",
    "for lemma in correct_per_word.keys():\n",
    "    if len(correct_per_word[lemma]) == 0:\n",
    "        accuracy_per_word[lemma] = \"NA\"\n",
    "    else:\n",
    "        mean = sum(correct_per_word[lemma]) / len(correct_per_word[lemma])\n",
    "        accuracy_per_word[lemma] = mean\n",
    "    \n",
    "print(\"=\"*40)\n",
    "print(\"EVALUATION\")\n",
    "print(f\"Overall accuracy: {round(accuracy, 3)}.\")\n",
    "print(\"Lemma{}\\tAcc.\\tBaseL.\\tGood?\\tNo. senses\".format(\" \"*9))\n",
    "\n",
    "##########################################\n",
    "# For interpretation of model performance, \n",
    "# I here collect variables for correlation\n",
    "def pearson(v1, v2):\n",
    "    calculation = np.corrcoef(v1, v2)\n",
    "    r = round(calculation[0][1], 3)\n",
    "    return r\n",
    "v_acc=[]\n",
    "v_bl=[]\n",
    "v_nsen=[]\n",
    "##########################################\n",
    "\n",
    "for lemma in accuracy_per_word.keys():\n",
    "    if lemma not in [\"<unk>\", \"<pad>\"]:\n",
    "        acc = round(accuracy_per_word[lemma], 2)\n",
    "        bl = round(my_baseline[lemma][\"accuracy\"], 2)\n",
    "        n_sense = my_baseline[lemma][\"no_of_senses\"]\n",
    "        is_it_good = \"Yes\"\n",
    "        if bl > acc:\n",
    "            is_it_good = \"No\"\n",
    "        \n",
    "        print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(lemma+\" \"*(14-len(lemma)), acc, bl, is_it_good, n_sense))\n",
    "        \n",
    "        v_acc.append(acc)\n",
    "        v_bl.append(bl)\n",
    "        v_nsen.append(n_sense)\n",
    "print(\"=\"*40)\n",
    "print()\n",
    "print(\"Correlation of Accuracy and Baseline: {}\".format(pearson(v_acc, v_bl)))\n",
    "print(\"Correlation of Accuracy and No. of senses: {}\".format(pearson(v_acc, v_nsen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Running a transformer for WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the lab you'll try out the transformer, specifically the BERT model. For this we'll use the huggingface library (https://huggingface.co/).\n",
    "\n",
    "You can find the documentation for the BERT model here (https://huggingface.co/transformers/model_doc/bert.html) and a general usage guide here (https://huggingface.co/transformers/quickstart.html).\n",
    "\n",
    "What we're going to do is *fine-tune* the BERT model, i.e. update the weights of a pre-trained model. That is, we have a model that is trained on language modeling, but now we apply it to word sense disambiguation with the word representations it learnt from language modeling.\n",
    "\n",
    "We'll use the same data splits for training and testing as before, but this time you'll not use a torchtext dataloader. Rather now you create an iterator that collects N sentences (where N is the batch size) then use the BertTokenizer to transform the sentence into integers. For your dataloader, remember to:\n",
    "* Shuffle the data in each batch\n",
    "* Make sure you get a new iterator for each *epoch*\n",
    "* Create a vocabulary of *sense-labels* so you can calculate accuracy \n",
    "\n",
    "We then pass this batch into the BERT model and train as before. The BERT model will encode the sentence, then we send this encoded sentence into a prediction layer (you can either the the sentence-representation from bert, or the ambiguous word) like before and collect sense predictions.\n",
    "\n",
    "About the hyperparameters and training:\n",
    "* For BERT, usually a lower learning rate works best, between 0.0001-0.000001.\n",
    "* BERT takes alot of resources, running it on CPU will take ages, utilize the GPUs :)\n",
    "* Since BERT takes alot of resources, use a small batch size (4-8)\n",
    "* Computing the BERT representation, make sure you pass the mask\n",
    "\n",
    "**[10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: **0 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataloading, I'd siggest implementing something simpler in lines of: \n",
    "\n",
    "```\n",
    "for BATCH, BATCH_LABELS in dataset:\n",
    "    input = tokenizer.batch_encode_plus(...)\n",
    "    labels = BATCH_LABELS\n",
    "    yield input, labels\n",
    "```\n",
    "\n",
    "should take less memory as you don't need to save everything into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upgrade the transformers library on MLTGPU, you can do this (copy/pasted from Discord):\n",
    "\n",
    "In case other people run into the problems with transformers, we did a short guide to use the most up-to-date version 4.6.0 of the transformers, instead of the 2.2.0 version on mltgpu:\n",
    "\n",
    "1. Create an empty directory and use it to create a virtual environment:\n",
    "python -m venv </path/to/new/virtual/environment>\n",
    "\n",
    "2. Activate created environment:\n",
    "source <pathofenv>/bin/activate\n",
    "\n",
    "3. To run jupyter, install jupyter in venv and run this command (also in venv) (you only need to change <nameofenv> to your environment name)\n",
    "pip install jupyter\n",
    "python -m ipykernel install --user --name=<nameofenv>\n",
    "\n",
    "4. Install all necessary dependencies, we used:\n",
    " pip install torch\n",
    " pip install transformers\n",
    " pip install -Iv torchtext==0.4.0\n",
    "\n",
    "5. Run the notebook in the venv and open your notebook, change the kernel from the browser: Kernel>Change kernel><nameofenv>\n",
    "\n",
    "NOTE: we had to log out of mltgpu and change the port for the notebook to open(edited)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# BERT stuff ...\n",
    "from transformers import BertModel, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(tokenizer)\n",
    "BERT = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  4.8.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(\"Version: \", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO BE** *RE-WORKED SUMMER 2021*\n",
    "\n",
    "### Note by M.B. \n",
    "I have experienced problems when implementing the BertTokenizer. The `transformers` library on the MLTGPU server is version 2.2, which makes implementation a struggle. This has consequences for any further implementation, training and evaluation of the model that requires this fundamental step. \n",
    "\n",
    "1. The litterature on how to work with the `transformers` library is based on later versions than 2.2. I do not get the procedures described and exemplified to work in v. 2.2 (consider: https://huggingface.co/transformers/training.html). For example, calling the tokenizer yields the following error: `TypeError: 'BertTokenizer' object is not callable`, which is a known problem for verison prior v3 (https://github.com/huggingface/transformers/issues/5580). \n",
    "2. There is no `docs`for the 2.2 version of BERT on huggingface (https://huggingface.co/transformers/v2.2.0/model_doc/bert.html). \n",
    "\n",
    "On my laptop I come to a stage that I manage to preprocess the data quite well (code below), but I cannot start experiment with the training part, since this code does not run on MLTGPU. \n",
    "\n",
    "I have considered building a BERT-ish tokenizer with the functionality of the `transformers` v 2.2 library, but there is not enough time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT hyperp\n",
    "b_batch_size = 3\n",
    "b_learning_rate = 0.0001\n",
    "b_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE-WORK ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split original dataset\n",
    "create a vocabulary of sense labels\n",
    "\n",
    "create a fancier dataset (data to data; light version possible without class?)\n",
    "\n",
    "for e in epochs\n",
    "    iterator = dataloader(fancy dataset) with shuffling\n",
    "    \n",
    "    iterate over iterator in batches:\n",
    "        \n",
    "        input --> model\n",
    "        output vs. label\n",
    "    \n",
    "    organize dataset into batches\n",
    "\n",
    "\n",
    "----\n",
    "what about index of word?\n",
    "what about dimensionality?\n",
    "----\n",
    "how to create batches?\n",
    "----\n",
    "is it a problem to have class defined data?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 B\n",
      "3 C\n",
      "4 D\n",
      "1 A\n",
      "----------\n",
      "3 C\n",
      "2 B\n",
      "4 D\n",
      "1 A\n",
      "----------\n",
      "3 C\n",
      "1 A\n",
      "2 B\n",
      "4 D\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "my_list=[(1,\"A\") , (2, \"B\"), (3, \"C\"), (4, \"D\")]\n",
    "\n",
    "def dataloader(a_list):\n",
    "    random.shuffle(a_list)\n",
    "    for x in a_list:\n",
    "        yield x\n",
    "\n",
    "def dataloader2(a_list):\n",
    "    random.shuffle(a_list)\n",
    "    for x, y in a_list:\n",
    "        yield x, y\n",
    "\n",
    "for e in range(3):\n",
    "    mydl=dataloader(my_list)\n",
    "    for x, y in mydl:\n",
    "        print(x, y)\n",
    "    print(\"-\"*10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From file to Python\n",
    "import random\n",
    "\n",
    "def read_and_split(path_to_dataset = \"wsd_data/wsd_data.txt\", train_frac = 0.8):\n",
    "    with open(path_to_dataset, mode=\"r\") as f:\n",
    "        data=[tuple(example.split(\"\\t\")) for example in f.read().split(\"\\n\") if len(example.split(\"\\t\")) == 4]\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    n_train = int(len(data)*train_frac)\n",
    "    train=data[:n_train]\n",
    "    test=data[n_train:]\n",
    "    \n",
    "    labels = []\n",
    "    lemmas = []\n",
    "    for label, lemma, x, y in data:\n",
    "        if label not in labels:\n",
    "            labels.append(label)\n",
    "        if lemma not in lemmas:\n",
    "            lemmas.append(lemma)\n",
    "    \n",
    "    return train, test, labels, lemmas\n",
    "    \n",
    "my_train, my_test, my_labels, my_lemmas = read_and_split()\n",
    "#print(my_train[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_to_data(data, labels_set, lemmas_set):\n",
    "    container = []\n",
    "    for label, lemma, index, sentence in data:\n",
    "        container.append((set_labels.index(label), \n",
    "                          set_lemmas.index(lemma), \n",
    "                          int(position), \n",
    "                          sentence))\n",
    "    return container\n",
    "\n",
    "fancy_train_data = data_to_data(my_train, my_labels, my_lemmas)\n",
    "fancy_test_data = data_to_data(my_test, my_labels, my_lemmas)\n",
    "\n",
    "def dataloader(data_as_list, batch_size):\n",
    "    random.shuffle(data_as_list)\n",
    "    \n",
    "    batched=[]\n",
    "    for b in [[data_as_list[i : i+batch_size]] for i in range(0, len(data_as_list), batch_size]:\n",
    "        labels=[]\n",
    "        sentences=[]\n",
    "        ... = []\n",
    "        for la, le, ... in b:\n",
    "            labels.append(la)\n",
    "            ...\n",
    "        batched.append((labels, sentences, ...))\n",
    "    \n",
    "    # batching ...\n",
    "    # output=[Batch(my_list[i : i+size]) for i in range(0, len(my_list), size)] \n",
    "    # solution found here: https://www.delftstack.com/howto/python/python-split-list-into-chunks/\n",
    "    \n",
    "    for label, lemma, index, sentence in batched: # do we do batching here?\n",
    "        \n",
    "        tok = tokenizer.batch_encode_plus(...)\n",
    "        \n",
    "        yield tok, label, ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW TOKENIZER\n",
    "\n",
    "for BATCH, BATCH_LABELS in dataset:\n",
    "    input = tokenizer.batch_encode_plus(...)\n",
    "    labels = BATCH_LABELS\n",
    "    yield input, labels\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_bert = 100 #dummy\n",
    "\n",
    "class BERT_WSD(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BERT_WSD, self).__init__()\n",
    "        self.bert = BERT\n",
    "        self.classifier = nn.Linear(from_bert, num_labels) #sentence repr or ambigious word repr ---> labels\n",
    "    \n",
    "    def forward(self, batch): #shall we use index?\n",
    "        #print(batch)      \n",
    "        \n",
    "        output = self.bert(**batch) #what do we get out from BERT?\n",
    "        #print(output)\n",
    "        predictions = self.classifier(output) \n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = BERT_WSD(len(my_labels))\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=b_learning_rate)\n",
    "\n",
    "total_loss = 0\n",
    "for e in range(b_epochs):\n",
    "    \n",
    "    iterator = dataloader(fancy_train_data, batch_size=...) ... #####\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        sentence = batch[...]\n",
    "        index = batch[...]\n",
    "        label = batch[...]\n",
    "        \n",
    "        #print(batch.position)\n",
    "      \n",
    "        output_from_model = model(sentence) #index?\n",
    "        \n",
    "        loss = loss_function(output_from_model, label.squeeze()) # Do we need squeeze for this version?\n",
    "        # \"output\" from model is \"input\" to CEL\n",
    "        \n",
    "        #Note: code below adopted from previous assignment\n",
    "        total_loss += loss.item()\n",
    "        print(i, total_loss/(i+1), end='\\r') \n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # update parameters\n",
    "        optimizer.zero_grad # reset gradients\n",
    "        \n",
    "        #break\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model after all epochs are completed\n",
    "def select(vector):\n",
    "    \"\"\"Selects the index of the top value in a vector.\"\"\"\n",
    "    top_value=0\n",
    "    no_one=0 #index of top value\n",
    "    for index, value in enumerate(vector):\n",
    "        if value > top_value:\n",
    "            top_value=value\n",
    "            no_one=index\n",
    "    return no_one   \n",
    "\n",
    "correct_set = []\n",
    "correct_per_word = {lemma:[] for lemma in my_lemmas}\n",
    "model.eval() #evaluation mode\n",
    "\n",
    "for i, batch in enumerate(test_iter):\n",
    "    print(f\"{round((i/len(test_iter))*100, 3)} %\", end=\"\\r\")\n",
    "    #Attributes have different names for bert_wsd part, than in first part ...\n",
    "    sentence = batch[...]\n",
    "    index    = batch[...]\n",
    "    label    = batch[...]\n",
    "    lemma    = batch[...]\n",
    "    \n",
    "    output = model(sentence, index) # index ...\n",
    "    \n",
    "    my_probs = F.softmax(output, dim=1)\n",
    "    index_of_top_prob = [select(x) for x in my_probs]\n",
    "    predicted_label = [my_labels[x] for x in index_of_top_prob]\n",
    "\n",
    "    for i in range(label.shape[0]):\n",
    "        true_label = my_labels[label[i][0]]\n",
    "        this_lemma = my_lemmas[lemma[i][0]]\n",
    "        if true_label == predicted_label[i]:\n",
    "            correct_set.append(1)\n",
    "            correct_per_word[this_lemma].append(1)\n",
    "        else:\n",
    "            correct_set.append(0)\n",
    "            correct_per_word[this_lemma].append(0)\n",
    "\n",
    "accuracy = sum(correct_set) / len(correct_set)\n",
    "\n",
    "accuracy_per_word = {lemma:0 for lemma in correct_per_word.keys()}\n",
    "for lemma in correct_per_word.keys():\n",
    "    if len(correct_per_word[lemma]) == 0:\n",
    "        accuracy_per_word[lemma] = \"NA\"\n",
    "    else:\n",
    "        mean = sum(correct_per_word[lemma]) / len(correct_per_word[lemma])\n",
    "        accuracy_per_word[lemma] = mean\n",
    "    \n",
    "print(\"=\"*40)\n",
    "print(\"EVALUATION\")\n",
    "print(f\"Overall accuracy: {round(accuracy, 3)}.\")\n",
    "print(\"Lemma{}\\tAcc.\\tBaseL.\\tGood?\".format(\" \"*9))\n",
    "for lemma in accuracy_per_word.keys():\n",
    "    if lemma not in [\"<unk>\", \"<pad>\"]:\n",
    "        acc = round(accuracy_per_word[lemma], 2)\n",
    "        bl = round(my_baseline[lemma][\"accuracy\"], 2)\n",
    "        is_it_good = \"Yes\"\n",
    "        if bl > acc:\n",
    "            is_it_good = \"No\"\n",
    "        \n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(lemma+\" \"*(14-len(lemma)), acc, bl, is_it_good))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VVVVV  OLD  VVVVV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLD >>>**\n",
    "```\n",
    "import random\n",
    "\n",
    "class Xemplar(): \n",
    "    def __init__(self, label, lemma, position, sentence, set_labels, set_lemmas):\n",
    "        self.label    = set_labels.index(label) \n",
    "        self.lemma    = set_lemmas.index(lemma) \n",
    "        self.position = int(position)\n",
    "        self.sentence = sentence.split(\" \")\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, chunk): #gets a list of len=batch_size of Xemplars \n",
    "        self.label    = torch.tensor([X.label for X in chunk], dtype=torch.long, device=device)\n",
    "        self.lemma    = torch.tensor([X.lemma for X in chunk], dtype=torch.long, device=device)\n",
    "        self.position = torch.tensor([X.position for X in chunk], dtype=torch.long, device=device)\n",
    "        self.sentence = tokenizer([X.sentence for X in chunk], \n",
    "                                     is_split_into_words=True, \n",
    "                                     padding=True, \n",
    "                                     truncation=True\n",
    "                                          )\n",
    "\n",
    "def data_to_data(data, labels_set, lemmas_set):\n",
    "    container = []\n",
    "    for label, lemma, index, sentence in data:\n",
    "        X = Xemplar(label, lemma, index, sentence, labels_set, lemmas_set)\n",
    "        container.append(X)    \n",
    "    return container\n",
    "\n",
    "def batcher(my_list, size):\n",
    "    output=[Batch(my_list[i : i+size]) for i in range(0, len(my_list), size)] #solution found here: https://www.delftstack.com/howto/python/python-split-list-into-chunks/\n",
    "    print(output)\n",
    "    return output\n",
    "        \n",
    "class MyDataLoader():\n",
    "    def __init__(self, data, labels_set, lemmas_set, batch_size=1, shuffle=True):\n",
    "        self.data       = data_to_data(data, labels_set, lemmas_set)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle==True:\n",
    "            random.shuffle(self.data)\n",
    "        if self.batch_size > 1:\n",
    "            iterator_to_be = batcher(self.data, size = self.batch_size)\n",
    "        else:\n",
    "            iterator_to_be = self.data\n",
    "        return iter(iterator_to_be) \n",
    "\n",
    "# So why does you not use Pytorch DataLoader, you might wonder ... This class seems to be retriced to two \n",
    "# aspects of the dataset (input and label), but I want four (label=sense, lemma, index of ambigious word,\n",
    "# and the sentence [or context]). Perhaps there are ways to use DataLoader in a less retricted way, but I\n",
    "# give up finding that functionality for now. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dataloader\n",
    "import random\n",
    "\n",
    "class Xemplar(): \n",
    "    def __init__(self, label, lemma, position, sentence, set_labels, set_lemmas):\n",
    "        self.label    = set_labels.index(label) \n",
    "        self.lemma    = set_lemmas.index(lemma) \n",
    "        self.position = int(position)\n",
    "        self.sentence = sentence.split(\" \")\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, chunk): #gets a list of len=batch_size of Xemplars \n",
    "        self.label    = torch.tensor([X.label for X in chunk], dtype=torch.long, device=device)\n",
    "        self.lemma    = torch.tensor([X.lemma for X in chunk], dtype=torch.long, device=device)\n",
    "        self.position = torch.tensor([X.position for X in chunk], dtype=torch.long, device=device)\n",
    "        self.sentence = tokenizer([X.sentence for X in chunk], \n",
    "                                     is_split_into_words=True, \n",
    "                                     padding=True, \n",
    "                                     truncation=True\n",
    "                                          )\n",
    "\n",
    "def data_to_data(data, labels_set, lemmas_set):\n",
    "    container = []\n",
    "    for label, lemma, index, sentence in data:\n",
    "        X = Xemplar(label, lemma, index, sentence, labels_set, lemmas_set)\n",
    "        container.append(X)    \n",
    "    return container\n",
    "\n",
    "def batcher(my_list, size):\n",
    "    output=[Batch(my_list[i : i+size]) for i in range(0, len(my_list), size)] #solution found here: https://www.delftstack.com/howto/python/python-split-list-into-chunks/\n",
    "    print(output)\n",
    "    return output\n",
    "        \n",
    "class MyDataLoader():\n",
    "    def __init__(self, data, labels_set, lemmas_set, batch_size=1, shuffle=True):\n",
    "        self.data       = data_to_data(data, labels_set, lemmas_set)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle==True:\n",
    "            random.shuffle(self.data)\n",
    "        if self.batch_size > 1:\n",
    "            iterator_to_be = batcher(self.data, size = self.batch_size)\n",
    "        else:\n",
    "            iterator_to_be = self.data\n",
    "        return iter(iterator_to_be) \n",
    "\n",
    "# So why does you not use Pytorch DataLoader, you might wonder ... This class seems to be retriced to two \n",
    "# aspects of the dataset (input and label), but I want four (label=sense, lemma, index of ambigious word,\n",
    "# and the sentence [or context]). Perhaps there are ways to use DataLoader in a less retricted way, but I\n",
    "# give up finding that functionality for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From data to dataloaders\n",
    "#my_train = my_train[:100]\n",
    "#my_test  = my_test[:100]\n",
    "\n",
    "b_train_iter = MyDataLoader(my_train, my_labels, my_lemmas, batch_size=b_batch_size)\n",
    "b_test_iter  = MyDataLoader(my_test, my_labels, my_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental phase, so far ... \n",
    "\n",
    "from_bert = 100 #dummy\n",
    "\n",
    "class BERT_WSD(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BERT_WSD, self).__init__()\n",
    "        self.bert = BERT\n",
    "        self.classifier = nn.Linear(from_bert, num_labels) #sentence repr or ambigious word repr ---> labels\n",
    "    \n",
    "    def forward(self, batch): #shall we use index?\n",
    "        #print(batch)      \n",
    "        \n",
    "        output = self.bert(**batch) #what do we get out from BERT?\n",
    "        #print(output)\n",
    "        predictions = self.classifier(output) \n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2047/3223145886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_train_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2047/3526822026.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0miterator_to_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0miterator_to_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2047/3526822026.py\u001b[0m in \u001b[0;36mbatcher\u001b[0;34m(my_list, size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#solution found here: https://www.delftstack.com/howto/python/python-split-list-into-chunks/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2047/3526822026.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#solution found here: https://www.delftstack.com/howto/python/python-split-list-into-chunks/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2047/3526822026.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, chunk)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                      \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                      \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                      \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                                           )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2325\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m             )\n\u001b[1;32m   2329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2510\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2512\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2513\u001b[0m         )\n\u001b[1;32m   2514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                     tokens = list(\n\u001b[0;32m--> 517\u001b[0;31m                         \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                     )\n\u001b[1;32m    519\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                     tokens = list(\n\u001b[0;32m--> 517\u001b[0;31m                         \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                     )\n\u001b[1;32m    519\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Simple mapping string => AddedToken for special tokens with specific tokenization behaviors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         all_special_tokens_extended = dict(\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens_extended\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mall_special_tokens_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0mall_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0mset_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_map_extended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mall_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_toks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ve_second/lib64/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mspecial_tokens_map_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mset_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspecial_tokens_map_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "    \n",
    "import torch.optim as optim\n",
    "\n",
    "model = BERT_WSD(len(my_labels))\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=b_learning_rate)\n",
    "\n",
    "total_loss = 0\n",
    "for e in range(b_epochs):\n",
    "    for i, batch in enumerate(b_train_iter):\n",
    "        \n",
    "        sentence = batch.sentence\n",
    "        index = batch.position\n",
    "        label = batch.label\n",
    "        \n",
    "        #print(batch.position)\n",
    "      \n",
    "        output_from_model = model(sentence) #index?\n",
    "        \n",
    "        loss = loss_function(output_from_model, label.squeeze()) # Do we need squeeze for this version?\n",
    "        # \"output\" from model is \"input\" to CEL\n",
    "        \n",
    "        #Note: code below adopted from previous assignment\n",
    "        total_loss += loss.item()\n",
    "        print(i, total_loss/(i+1), end='\\r') \n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # update parameters\n",
    "        optimizer.zero_grad # reset gradients\n",
    "        \n",
    "        #break\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model after all epochs are completed\n",
    "def select(vector):\n",
    "    \"\"\"Selects the index of the top value in a vector.\"\"\"\n",
    "    top_value=0\n",
    "    no_one=0 #index of top value\n",
    "    for index, value in enumerate(vector):\n",
    "        if value > top_value:\n",
    "            top_value=value\n",
    "            no_one=index\n",
    "    return no_one   \n",
    "\n",
    "correct_set = []\n",
    "correct_per_word = {lemma:[] for lemma in my_lemmas}\n",
    "model.eval() #evaluation mode\n",
    "\n",
    "for i, batch in enumerate(test_iter):\n",
    "    print(f\"{round((i/len(test_iter))*100, 3)} %\", end=\"\\r\")\n",
    "    #Attributes have different names for bert_wsd part, than in first part ...\n",
    "    sentence = batch.sentence\n",
    "    index    = batch.position\n",
    "    label    = batch.label\n",
    "    lemma    = batch.lemma\n",
    "    \n",
    "    output = model(sentence, index) # index ...\n",
    "    \n",
    "    my_probs = F.softmax(output, dim=1)\n",
    "    index_of_top_prob = [select(x) for x in my_probs]\n",
    "    predicted_label = [my_labels[x] for x in index_of_top_prob]\n",
    "\n",
    "    for i in range(label.shape[0]):\n",
    "        true_label = my_labels[label[i][0]]\n",
    "        this_lemma = my_lemmas[lemma[i][0]]\n",
    "        if true_label == predicted_label[i]:\n",
    "            correct_set.append(1)\n",
    "            correct_per_word[this_lemma].append(1)\n",
    "        else:\n",
    "            correct_set.append(0)\n",
    "            correct_per_word[this_lemma].append(0)\n",
    "\n",
    "accuracy = sum(correct_set) / len(correct_set)\n",
    "\n",
    "accuracy_per_word = {lemma:0 for lemma in correct_per_word.keys()}\n",
    "for lemma in correct_per_word.keys():\n",
    "    if len(correct_per_word[lemma]) == 0:\n",
    "        accuracy_per_word[lemma] = \"NA\"\n",
    "    else:\n",
    "        mean = sum(correct_per_word[lemma]) / len(correct_per_word[lemma])\n",
    "        accuracy_per_word[lemma] = mean\n",
    "    \n",
    "print(\"=\"*40)\n",
    "print(\"EVALUATION\")\n",
    "print(f\"Overall accuracy: {round(accuracy, 3)}.\")\n",
    "print(\"Lemma{}\\tAcc.\\tBaseL.\\tGood?\".format(\" \"*9))\n",
    "for lemma in accuracy_per_word.keys():\n",
    "    if lemma not in [\"<unk>\", \"<pad>\"]:\n",
    "        acc = round(accuracy_per_word[lemma], 2)\n",
    "        bl = round(my_baseline[lemma][\"accuracy\"], 2)\n",
    "        is_it_good = \"Yes\"\n",
    "        if bl > acc:\n",
    "            is_it_good = \"No\"\n",
    "        \n",
    "        print(\"{}\\t{}\\t{}\\t{}\".format(lemma+\" \"*(14-len(lemma)), acc, bl, is_it_good))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the difference between the first and second approach. What kind of representations are the different approaches using to predict word-senses? **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Yeah! **4 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The first approach attempts to classify meaning from the representation of *the ambiguous word, as it appears in a sequence*. The second approach attempts to classify the meaning of the ambiguous word based on the representation of *the sentence* (in which the ambiguous word appears). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model with per-word-form *accuracy* and comment on the results you get, how does the model perform in comparison to the baseline, and how do the models compare to each other? \n",
    "\n",
    "Expand on the evaluation by sorting the word-forms by the number of senses they have. Are word-forms with fewer senses easier to predict? Give a short explanation of the results you get based on the number of senses per word.\n",
    "\n",
    "**[6 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Good analysis, and I agree with all your points! **6 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "    model = WSDModel_approach1()\n",
    "    a_batch_size = 16\n",
    "    a_learning_rate = 0.001\n",
    "    a_epochs = 8\n",
    "    a_hidden = 256\n",
    "\n",
    "### Results\n",
    "**Overall accuracy: 0.484.**\n",
    "\n",
    "**Table: Accuracy, Baseline, Improvement from Baseline, and No. of senses.**\n",
    "\n",
    "|Lemma         |Acc.|BaseL.|Good?|No. senses|\n",
    "|--------------|----|----|-------|----------|\n",
    "|see.v         |0.61|0.63|No|11|\n",
    "|line.n        |0.92|0.85|Yes|11|\n",
    "|keep.v        |0.54|0.39|Yes|11|\n",
    "|follow.v      |0.46|0.15|Yes|11|\n",
    "|hold.v        |0.33|0.15|Yes|11|\n",
    "|serve.v       |0.38|0.16|Yes|9|\n",
    "|force.n       |0.61|0.16|Yes|8|\n",
    "|lead.v        |0.33|0.18|Yes|8|\n",
    "|build.v       |0.28|0.21|Yes|10|\n",
    "|bring.v       |0.29|0.21|Yes|8|\n",
    "|extend.v      |0.35|0.18|Yes|7|\n",
    "|find.v        |0.43|0.23|Yes|10|\n",
    "|case.n        |0.34|0.2|Yes|8|\n",
    "|position.n    |0.27|0.2|Yes|6|\n",
    "|national.a    |0.41|0.2|Yes|6|\n",
    "|security.n    |0.57|0.2|Yes|7|\n",
    "|life.n        |0.51|0.22|Yes|9|\n",
    "|time.n        |0.5|0.28|Yes|5|\n",
    "|professional.a|0.57|0.22|Yes|5|\n",
    "|order.n       |0.52|0.22|Yes|5|\n",
    "|regular.a     |0.39|0.22|Yes|8|\n",
    "|point.n       |0.44|0.36|Yes|8|\n",
    "|place.n       |0.48|0.24|Yes|7|\n",
    "|physical.a    |0.32|0.24|Yes|6|\n",
    "|common.a      |0.39|0.25|Yes|4|\n",
    "|bad.a         |0.68|0.61|Yes|4|\n",
    "|critical.a    |0.45|0.27|Yes|5|\n",
    "|major.a       |0.42|0.3|Yes|4|\n",
    "|active.a      |0.44|0.32|Yes|5|\n",
    "|positive.a    |0.49|0.35|Yes|5|\n",
    "\n",
    "**Correlation of Accuracy and Baseline: 0.753**\n",
    "\n",
    "**Correlation of Accuracy and No. of senses: 0.091**\n",
    "\n",
    "### Conclusion & Discussion\n",
    "From this data we can draw the following conclusions:\n",
    "\n",
    "*   Overall accuracy is 48.4 which is not strong, but far from worthless, given the complexity of the task at hand. Kågebäck & Salomonsson reported substantially higher scores (66.9 for SE2 and 73.4 for SE3), but still leaving room for improvement. \n",
    "*   The neural model is better than the baseline for every lemma except one (*see*). We might specualte that *see* causes special for the model given its subtile (metaphoric) variation of meaning. Senses of *see* might be hard to clearly to distingusih for humans as well.   \n",
    "*   Predictions does not get better, with fewer senses of lemmas, or worse with many senses of lemmas. Pearson's correlation coefficient is close to 0 between Accuracy and No. of senses. (Of course, there would be a lower limit for this dissociation as No. of senses = 1, would yield 100% accuracy). \n",
    "*   There is however a strong correlation between Accuracy and Basline. This suggests that the model is better at predicting *dominant* senses. As the basline is determined by the most common sense of a word, words with high baselines are words where a large proportions of its tokens encode the \"baseline sense\". The model is especially good at predciting such common senses, seemingly independent of the number of other senses there are for the word.  \n",
    "*   There are some interesting exceptions to the previous point. For *force*, the model makes quite good predicitons, although this is a lemma without a clear dominant sense.Similarily, the model disambiguates *security* quite well despite its lack of a dominant sense. We might hypothize that these words have fairly distinct sense, appearing in quite different contexts, making them easier for the model to distingusih and recognize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the LSTMs perform in comparison to BERT? What's the difference between representations obtained by the LSTMs and BERT? **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: **0 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RE-WORK SUMMER 2021*\n",
    "\n",
    "**Answer:** Unfortunately, I have not been able to build and train (fine-tune) the `BERT_WSD` model (see comment above and `readme.md` file). Therefore, there is no comparison to make. However, something can be said about what the models are (supposed) to represent. The LSTM model represents tokens relative a sequence, so that representations of previously processed elements are \"remembered\" at the current state. BERT represents something different. It represents words and sentences at several levels (based on attention). When finetuned, the general \"knowledge\" of BERT is calibrated on a particular task; here: WSD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could we do to improve our LSTM word sense disambiguation models and our BERT model? **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AE: Good suggestions, it would be helpful if BERT worked indeed :D **2 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "For the LSTM model, we can as Kågebäck & Salomonsson try to \n",
    "*   parameterize sense by word type (not trainging every sense for every word, as above)\n",
    "*   use the dropword technique\n",
    "*   use dropout on layer\n",
    "*   use a pretrained word embedding (e.g. Glove)\n",
    "\n",
    "For BERT, make it work :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings:\n",
    "\n",
    "[1] Kågebäck, M., & Salomonsson, H. (2016). Word Sense Disambiguation using a Bidirectional LSTM. arXiv preprint arXiv:1606.03568.\n",
    "\n",
    "[2] https://cl.lingfil.uu.se/~nivre/master/NLP-LexSem.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total marks: 26, which unfourtunately is not enought to pass. Your analysis and general code is good, but there are some problems in the model implementations. There will be a deadline in September for resubmission (date will be announced soon-ish). If you have any questions regarding the code or so, feel free to hit me up on e-mail or Discord!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_second",
   "language": "python",
   "name": "ve_second"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
